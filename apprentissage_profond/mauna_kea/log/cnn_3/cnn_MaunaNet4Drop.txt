MaunaNet4Drop(
  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop1): Dropout2d(p=0.2)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop2): Dropout2d(p=0.2)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop3): Dropout2d(p=0.2)
  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop4): Dropout2d(p=0.2)
  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (fc1): Linear(in_features=256, out_features=4, bias=True)
)
train patients [42 40 24 53  2 55 22 17 29 18 51  8 14  5 19 43 15 46  7 45  1 34 13 44
 31  6 30 47 25 23 10 50 54  3  0]
train labels [1111 2105  798 3241]
test patients [41 48 35  4 11 32 36 12 49]
test labels [ 358 1072  408  353]

[train epoch 1/50] | loss 1.1171 | nw acc 0.518 | time 2 min 17 sec
cat 0: [404, 71, 71, 171] and [0.5635, 0.099, 0.099, 0.2385]
cat 1: [253, 1064, 266, 744] and [0.1087, 0.4572, 0.1143, 0.3197]
cat 2: [9, 13, 3, 17] and [0.2143, 0.3095, 0.0714, 0.4048]
cat 3: [445, 957, 458, 2309] and [0.1067, 0.2296, 0.1099, 0.5538]
[test epoch 1/50] | loss 0.438 | nw acc 0.306 | time 0 min 23 sec
cat 0: [103, 74, 11, 10] and [0.5202, 0.3737, 0.0556, 0.0505]
cat 1: [135, 351, 113, 118] and [0.1883, 0.4895, 0.1576, 0.1646]
cat 2: [55, 3, 8, 1] and [0.8209, 0.0448, 0.1194, 0.0149]
cat 3: [65, 644, 276, 224] and [0.0538, 0.5327, 0.2283, 0.1853]
MaunaNet4Drop(
  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop1): Dropout2d(p=0.2)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop2): Dropout2d(p=0.2)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop3): Dropout2d(p=0.2)
  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop4): Dropout2d(p=0.2)
  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (fc1): Linear(in_features=256, out_features=4, bias=True)
)
train patients [42 40 24 53  2 55 22 17 29 18 51  8 14  5 19 43 15 46  7 45  1 34 13 44
 31  6 30 47 25 23 10 50 54  3  0]
train labels [1111 2105  798 3241]
test patients [41 48 35  4 11 32 36 12 49]
test labels [ 358 1072  408  353]

[train epoch 1/50] | loss 1.1178 | nw acc 0.493 | time 2 min 15 sec
cat 0: [387, 73, 43, 179] and [0.5674, 0.107, 0.063, 0.2625]
cat 1: [222, 1006, 311, 852] and [0.0928, 0.4207, 0.1301, 0.3563]
cat 2: [34, 45, 39, 46] and [0.2073, 0.2744, 0.2378, 0.2805]
cat 3: [468, 981, 405, 2164] and [0.1165, 0.2442, 0.1008, 0.5386]
[test epoch 1/50] | loss 0.419 | nw acc 0.29 | time 0 min 23 sec
cat 0: [74, 147, 36, 14] and [0.2731, 0.5424, 0.1328, 0.0517]
cat 1: [100, 276, 96, 174] and [0.1548, 0.4272, 0.1486, 0.2693]
cat 2: [135, 202, 159, 25] and [0.2591, 0.3877, 0.3052, 0.048]
cat 3: [49, 447, 117, 140] and [0.0651, 0.5936, 0.1554, 0.1859]
[train epoch 2/50] | loss 0.99378 | nw acc 0.594 | time 2 min 16 sec
cat 0: [624, 70, 86, 191] and [0.6426, 0.0721, 0.0886, 0.1967]
cat 1: [108, 1032, 173, 429] and [0.062, 0.5924, 0.0993, 0.2463]
cat 2: [42, 76, 137, 79] and [0.1257, 0.2275, 0.4102, 0.2365]
cat 3: [337, 927, 402, 2542] and [0.0801, 0.2203, 0.0955, 0.6041]
[test epoch 2/50] | loss 0.424 | nw acc 0.304 | time 0 min 23 sec
cat 0: [121, 110, 13, 2] and [0.4919, 0.4472, 0.0528, 0.0081]
cat 1: [44, 167, 62, 13] and [0.1538, 0.5839, 0.2168, 0.0455]
cat 2: [106, 111, 78, 24] and [0.3323, 0.348, 0.2445, 0.0752]
cat 3: [87, 684, 255, 314] and [0.0649, 0.5104, 0.1903, 0.2343]
[train epoch 3/50] | loss 0.89586 | nw acc 0.663 | time 2 min 16 sec
cat 0: [742, 53, 76, 153] and [0.7246, 0.0518, 0.0742, 0.1494]
cat 1: [78, 1264, 145, 374] and [0.0419, 0.6792, 0.0779, 0.201]
cat 2: [33, 59, 196, 78] and [0.0902, 0.1612, 0.5355, 0.2131]
cat 3: [258, 729, 381, 2636] and [0.0644, 0.1821, 0.0952, 0.6583]
[test epoch 3/50] | loss 0.458 | nw acc 0.309 | time 0 min 33 sec
cat 0: [207, 139, 26, 6] and [0.5476, 0.3677, 0.0688, 0.0159]
cat 1: [42, 164, 99, 19] and [0.1296, 0.5062, 0.3056, 0.0586]
cat 2: [25, 45, 10, 17] and [0.2577, 0.4639, 0.1031, 0.1753]
cat 3: [84, 724, 273, 311] and [0.0603, 0.5201, 0.1961, 0.2234]
[train epoch 4/50] | loss 0.83629 | nw acc 0.695 | time 2 min 16 sec
cat 0: [803, 68, 66, 134] and [0.7498, 0.0635, 0.0616, 0.1251]
cat 1: [51, 1325, 120, 326] and [0.028, 0.7272, 0.0659, 0.1789]
cat 2: [36, 44, 246, 87] and [0.0872, 0.1065, 0.5956, 0.2107]
cat 3: [221, 668, 366, 2694] and [0.056, 0.1692, 0.0927, 0.6822]
[test epoch 4/50] | loss 0.546 | nw acc 0.261 | time 0 min 23 sec
cat 0: [272, 433, 153, 41] and [0.3026, 0.4816, 0.1702, 0.0456]
cat 1: [0, 5, 8, 1] and [0.0, 0.3571, 0.5714, 0.0714]
cat 2: [0, 0, 0, 3] and [0.0, 0.0, 0.0, 1.0]
cat 3: [86, 634, 247, 308] and [0.0675, 0.4973, 0.1937, 0.2416]
[train epoch 5/50] | loss 0.78538 | nw acc 0.733 | time 2 min 17 sec
cat 0: [847, 57, 48, 127] and [0.785, 0.0528, 0.0445, 0.1177]
cat 1: [69, 1446, 121, 321] and [0.0353, 0.7389, 0.0618, 0.164]
cat 2: [23, 38, 333, 70] and [0.0496, 0.0819, 0.7177, 0.1509]
cat 3: [172, 564, 296, 2723] and [0.0458, 0.1502, 0.0788, 0.7252]
[test epoch 5/50] | loss 0.422 | nw acc 0.367 | time 0 min 26 sec
cat 0: [190, 123, 8, 15] and [0.5655, 0.3661, 0.0238, 0.0446]
cat 1: [79, 404, 249, 117] and [0.0931, 0.4759, 0.2933, 0.1378]
cat 2: [57, 370, 79, 73] and [0.0984, 0.639, 0.1364, 0.1261]
cat 3: [32, 175, 72, 148] and [0.0749, 0.4098, 0.1686, 0.3466]
[train epoch 6/50] | loss 0.7308 | nw acc 0.775 | time 2 min 16 sec
cat 0: [909, 54, 40, 112] and [0.8152, 0.0484, 0.0359, 0.1004]
cat 1: [48, 1537, 97, 272] and [0.0246, 0.7866, 0.0496, 0.1392]
cat 2: [17, 31, 402, 52] and [0.0339, 0.0618, 0.8008, 0.1036]
cat 3: [137, 483, 259, 2805] and [0.0372, 0.1311, 0.0703, 0.7614]
[test epoch 6/50] | loss 0.444 | nw acc 0.322 | time 0 min 25 sec
cat 0: [188, 95, 3, 16] and [0.6225, 0.3146, 0.0099, 0.053]
cat 1: [45, 346, 233, 43] and [0.0675, 0.5187, 0.3493, 0.0645]
cat 2: [74, 558, 167, 273] and [0.069, 0.5205, 0.1558, 0.2547]
cat 3: [51, 73, 5, 21] and [0.34, 0.4867, 0.0333, 0.14]
[train epoch 7/50] | loss 0.69791 | nw acc 0.794 | time 2 min 16 sec
cat 0: [904, 53, 39, 93] and [0.8301, 0.0487, 0.0358, 0.0854]
cat 1: [55, 1624, 80, 255] and [0.0273, 0.8064, 0.0397, 0.1266]
cat 2: [15, 40, 453, 80] and [0.0255, 0.068, 0.7704, 0.1361]
cat 3: [137, 388, 226, 2813] and [0.0384, 0.1089, 0.0634, 0.7893]
[test epoch 7/50] | loss 0.447 | nw acc 0.32 | time 0 min 24 sec
cat 0: [116, 47, 1, 6] and [0.6824, 0.2765, 0.0059, 0.0353]
cat 1: [27, 319, 228, 58] and [0.0427, 0.5047, 0.3608, 0.0918]
cat 2: [137, 396, 117, 124] and [0.177, 0.5116, 0.1512, 0.1602]
cat 3: [78, 310, 62, 165] and [0.1268, 0.5041, 0.1008, 0.2683]
[train epoch 8/50] | loss 0.6502 | nw acc 0.829 | time 11 min 38 sec
cat 0: [947, 33, 29, 92] and [0.8601, 0.03, 0.0263, 0.0836]
cat 1: [38, 1702, 78, 227] and [0.0186, 0.8323, 0.0381, 0.111]
cat 2: [14, 32, 524, 47] and [0.0227, 0.0519, 0.8493, 0.0762]
cat 3: [112, 338, 167, 2875] and [0.0321, 0.0968, 0.0478, 0.8233]
[test epoch 8/50] | loss 0.448 | nw acc 0.352 | time 0 min 19 sec
cat 0: [116, 41, 0, 1] and [0.7342, 0.2595, 0.0, 0.0063]
cat 1: [46, 384, 247, 118] and [0.0579, 0.483, 0.3107, 0.1484]
cat 2: [110, 412, 135, 80] and [0.1493, 0.559, 0.1832, 0.1085]
cat 3: [86, 235, 26, 154] and [0.1717, 0.4691, 0.0519, 0.3074]
[train epoch 9/50] | loss 0.6239 | nw acc 0.85 | time 2 min 31 sec
cat 0: [956, 33, 28, 73] and [0.8771, 0.0303, 0.0257, 0.067]
cat 1: [38, 1763, 54, 212] and [0.0184, 0.8529, 0.0261, 0.1026]
cat 2: [9, 34, 575, 50] and [0.0135, 0.0509, 0.8608, 0.0749]
cat 3: [108, 275, 141, 2906] and [0.0315, 0.0802, 0.0411, 0.8472]
[test epoch 9/50] | loss 0.427 | nw acc 0.392 | time 0 min 24 sec
cat 0: [170, 160, 13, 16] and [0.4735, 0.4457, 0.0362, 0.0446]
cat 1: [111, 607, 326, 218] and [0.088, 0.481, 0.2583, 0.1727]
cat 2: [53, 192, 36, 55] and [0.1577, 0.5714, 0.1071, 0.1637]
cat 3: [24, 113, 33, 64] and [0.1026, 0.4829, 0.141, 0.2735]
[train epoch 10/50] | loss 0.60905 | nw acc 0.857 | time 2 min 24 sec
cat 0: [949, 37, 25, 70] and [0.8779, 0.0342, 0.0231, 0.0648]
cat 1: [43, 1794, 65, 211] and [0.0204, 0.849, 0.0308, 0.0999]
cat 2: [8, 32, 597, 49] and [0.0117, 0.0466, 0.8703, 0.0714]
cat 3: [111, 242, 111, 2911] and [0.0329, 0.0717, 0.0329, 0.8625]
[test epoch 10/50] | loss 0.405 | nw acc 0.438 | time 0 min 26 sec
cat 0: [192, 117, 5, 9] and [0.5944, 0.3622, 0.0155, 0.0279]
cat 1: [61, 548, 297, 125] and [0.0592, 0.5315, 0.2881, 0.1212]
cat 2: [45, 177, 71, 50] and [0.1312, 0.516, 0.207, 0.1458]
cat 3: [60, 230, 35, 169] and [0.1215, 0.4656, 0.0709, 0.3421]
[train epoch 11/50] | loss 0.57073 | nw acc 0.879 | time 2 min 23 sec
cat 0: [970, 41, 22, 58] and [0.8891, 0.0376, 0.0202, 0.0532]
cat 1: [44, 1849, 51, 167] and [0.0208, 0.8759, 0.0242, 0.0791]
cat 2: [5, 23, 632, 53] and [0.007, 0.0323, 0.8864, 0.0743]
cat 3: [92, 192, 93, 2963] and [0.0275, 0.0575, 0.0278, 0.8871]
[test epoch 11/50] | loss 0.478 | nw acc 0.285 | time 0 min 26 sec
cat 0: [98, 87, 0, 7] and [0.5104, 0.4531, 0.0, 0.0365]
cat 1: [29, 331, 239, 135] and [0.0395, 0.451, 0.3256, 0.1839]
cat 2: [57, 195, 85, 86] and [0.1348, 0.461, 0.2009, 0.2033]
cat 3: [174, 459, 84, 125] and [0.2067, 0.5451, 0.0998, 0.1485]
[train epoch 12/50] | loss 0.55062 | nw acc 0.892 | time 2 min 19 sec
cat 0: [991, 34, 18, 62] and [0.8968, 0.0308, 0.0163, 0.0561]
cat 1: [31, 1866, 37, 144] and [0.0149, 0.898, 0.0178, 0.0693]
cat 2: [6, 25, 648, 31] and [0.0085, 0.0352, 0.9127, 0.0437]
cat 3: [83, 180, 95, 3004] and [0.0247, 0.0535, 0.0283, 0.8935]
[test epoch 12/50] | loss 0.444 | nw acc 0.348 | time 0 min 23 sec
cat 0: [116, 94, 0, 8] and [0.5321, 0.4312, 0.0, 0.0367]
cat 1: [57, 491, 295, 165] and [0.0565, 0.4871, 0.2927, 0.1637]
cat 2: [122, 384, 108, 115] and [0.1674, 0.5267, 0.1481, 0.1578]
cat 3: [63, 103, 5, 65] and [0.2669, 0.4364, 0.0212, 0.2754]
[train epoch 13/50] | loss 0.53303 | nw acc 0.903 | time 2 min 22 sec
cat 0: [986, 27, 12, 65] and [0.9046, 0.0248, 0.011, 0.0596]
cat 1: [33, 1912, 39, 119] and [0.0157, 0.9092, 0.0185, 0.0566]
cat 2: [5, 13, 666, 31] and [0.007, 0.0182, 0.9315, 0.0434]
cat 3: [87, 153, 81, 3026] and [0.026, 0.0457, 0.0242, 0.9041]
[test epoch 13/50] | loss 0.428 | nw acc 0.389 | time 0 min 28 sec
cat 0: [133, 62, 0, 2] and [0.6751, 0.3147, 0.0, 0.0102]
cat 1: [54, 526, 316, 144] and [0.0519, 0.5058, 0.3038, 0.1385]
cat 2: [50, 95, 34, 28] and [0.2415, 0.4589, 0.1643, 0.1353]
cat 3: [121, 389, 58, 179] and [0.162, 0.5207, 0.0776, 0.2396]
[train epoch 14/50] | loss 0.50554 | nw acc 0.917 | time 2 min 20 sec
cat 0: [1014, 24, 11, 49] and [0.9235, 0.0219, 0.01, 0.0446]
cat 1: [29, 1919, 24, 103] and [0.014, 0.9248, 0.0116, 0.0496]
cat 2: [5, 19, 701, 34] and [0.0066, 0.025, 0.9236, 0.0448]
cat 3: [63, 143, 62, 3055] and [0.019, 0.043, 0.0187, 0.9193]
[test epoch 14/50] | loss 0.446 | nw acc 0.348 | time 0 min 29 sec
cat 0: [121, 151, 1, 10] and [0.4276, 0.5336, 0.0035, 0.0353]
cat 1: [26, 372, 229, 74] and [0.0371, 0.5307, 0.3267, 0.1056]
cat 2: [125, 370, 164, 146] and [0.1553, 0.4596, 0.2037, 0.1814]
cat 3: [86, 179, 14, 123] and [0.2139, 0.4453, 0.0348, 0.306]
[train epoch 15/50] | loss 0.5051 | nw acc 0.917 | time 2 min 27 sec
cat 0: [1015, 26, 12, 44] and [0.9253, 0.0237, 0.0109, 0.0401]
cat 1: [34, 1929, 31, 102] and [0.0162, 0.9203, 0.0148, 0.0487]
cat 2: [6, 14, 686, 32] and [0.0081, 0.019, 0.9295, 0.0434]
cat 3: [56, 136, 69, 3063] and [0.0168, 0.0409, 0.0208, 0.9215]
[test epoch 15/50] | loss 0.477 | nw acc 0.321 | time 0 min 34 sec
cat 0: [185, 305, 55, 39] and [0.3168, 0.5223, 0.0942, 0.0668]
cat 1: [47, 425, 293, 192] and [0.0491, 0.4441, 0.3062, 0.2006]
cat 2: [68, 117, 27, 40] and [0.2698, 0.4643, 0.1071, 0.1587]
cat 3: [58, 225, 33, 82] and [0.1457, 0.5653, 0.0829, 0.206]
