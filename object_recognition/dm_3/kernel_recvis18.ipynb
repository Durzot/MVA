{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "###############################################\n# Functions used for preprocessing input images\n###############################################\n\nimport torchvision.transforms as transforms\n\n# once the images are loaded, how do we pre-process them before being passed into the network\n# and normalize them to mean = 0 and standard-deviation = 1\n\n# We also include data augmentation techniques as our dataset is quite small.  These include\n# RandomHorizontalFlip\n# These transformations are performed at each epoch\n\ndata_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n])\n\n\n###############################################\n# Main of the program\n###############################################\n\nimport argparse\nimport os\nimport numpy as np\nimport pandas as pd\n\npd.set_option(\"display.width\", 1000)\npd.set_option(\"display.max_columns\", 100)\npd.set_option(\"display.max_rows\", 600)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets\nfrom torch.autograd import Variable\nfrom tqdm import tqdm\nimport PIL.Image as Image\nos.system(\"! pip install cnn_finetune\")\nfrom cnn_finetune import make_model\nimport xgboost as xgb\n\nimport gc\ngc.enable()\n\n\n# Training settings\ndata_dir = \"../input/train-images-recvis18/train_images/train_images\"\nseed = 1\nbatch_size = 32\nlog_interval = 5\nexperiment = \"../working/experiment\"\ntest_dir = \"../input/mva-recvis-2018/bird_dataset/bird_dataset/test_images/mistery_category\"\nmomentum = 0.9\nnepoch = 10\nnclasses = 20 \nlr = 0.01\n\nuse_cuda = torch.cuda.is_available()\ntorch.manual_seed(seed)\n\n# Create experiment folder\nif not os.path.isdir(experiment):\n    os.makedirs(experiment)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e3100d23764e2921c2d81ade9cda48854b258eea"
      },
      "cell_type": "markdown",
      "source": "# 1. Define functions used for fitting models"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94d2a8b908b414af59aaff6a1834ce8d2ba7be3a"
      },
      "cell_type": "code",
      "source": "###################################################\n# Functions used to run models and make predictions\n###################################################\n\n#########################################################\n# Train and validation functions for CNN trained with SGD\n#########################################################\n\ndef train(epoch, model, train_loader, ntrain, batch_size):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        optimizer.zero_grad()\n        output = model(data)\n        criterion = torch.nn.CrossEntropyLoss(reduction='elementwise_mean')\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), ntrain,\n                100. * batch_idx / np.int(ntrain / batch_size), loss.data.item()))\n    return model\n\ndef validation(model, val_loader, nval):\n    model.eval()\n    validation_loss = 0\n    correct = 0\n    for data, target in val_loader:\n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        output = model(data)\n        # sum up batch loss\n        criterion = torch.nn.CrossEntropyLoss(reduction='elementwise_mean')\n        validation_loss += criterion(output, target).data.item()\n\n        # get the index of the max log-probability\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    validation_loss /= nval\n    accuracy = 100. * correct / nval\n    accuracy = accuracy.data.numpy()\n    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        validation_loss, correct, nval, accuracy))\n    return accuracy\n    \n\n###################################################################\n# Train function for classifier trained with batch gradient descent\n###################################################################\n\n\ndef train_batch(model, classifier, classifier_label, train_loader, val_loader, nval):\n    model.train()\n    \n    batches_data_train, batches_data_val = [], []\n    batches_target_train, batches_target_val = [], []\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        \n        # Compute output of pretrained model and store it in numpy format\n        output = model(data)\n        output = output.view(output.size()[0], -1).cpu().data.numpy()\n        batches_data_train.append(output)\n        \n        # Store target in numpy format\n        target = target.cpu().data.numpy()\n        batches_target_train.append(target)\n\n    for data, target in val_loader:\n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n\n        # Compute output of pretrained model and store it in numpy format\n        output = model(data)\n        output = output.view(output.size()[0], -1).cpu().data.numpy()\n        batches_data_val.append(output)\n        \n        # Store target in numpy format\n        target = target.cpu().data.numpy()\n        batches_target_val.append(target)\n        \n    data_train = np.concatenate(batches_data_train, axis=0)\n    target_train = np.concatenate(batches_target_train, axis=0)\n    data_val = np.concatenate(batches_data_val, axis=0)\n    target_val = np.concatenate(batches_target_val, axis=0)\n    \n    print(\"Whole train and validation loaded for classifier\\n\")\n    \n    if classifier_label == \"xgboost\":\n        xgb_train = xgb.DMatrix(data_train, label=target_train)\n        xgb_val = xgb.DMatrix(data_val, label=target_val)\n        \n        params = {\n            # Parameters that we are going to tune.\n            'booster': 'gbtree',\n            'n_estimators': 200,\n            'max_depth':15,\n            'min_child_weight': 1,\n            'learning_rate':.1,\n            'subsample': 0.7,\n            'colsample_bytree': 0.7,\n            'seed': 1,\n            # Other parameters\n            'objective': 'multi:softmax',\n            'num_class': nclasses,\n            'eval_metric': 'merror', \n            'silent': 1\n        }\n        \n        num_boost_round = 35\n        early_stopping_rounds = 5\n        \n        # # Number of folds for cross-validation of parameters\n        # nfold = 5\n        \n        # # Cross-validate max_depth/min_child_weight\n        # # These parameters control the complexity of the trees\n        # gridsearch_params = [(max_depth, min_child_weight) for max_depth in [8, 10, 15] for min_child_weight in [1,5,8]]\n        \n        # # Define initial best params and MAE\n        # min_merror = float(\"Inf\")\n        # best_params = None\n        \n        # for max_depth, min_child_weight in gridsearch_params:\n        #     print(\"CV with max_depth={}, min_child_weight={}\".format(max_depth, min_child_weight))\n            \n        #     # Update our parameters\n        #     params['max_depth'] = max_depth\n        #     params['min_child_weight'] = min_child_weight\n            \n        #     cv_results = xgb.cv(\n        #         params,\n        #         xgb_train,\n        #         num_boost_round=num_boost_round,\n        #         nfold=nfold,\n        #         early_stopping_rounds=early_stopping_rounds,\n        #         verbose_eval=True\n        #     )\n        \n    \n        #     # Update best MAE\n        #     mean_merror = cv_results['test-merror-mean'].min()\n        #     boost_rounds = cv_results['test-merror-mean'].argmin()\n            \n        #     print(\"\\tmerror {} for {} rounds\".format(mean_mae, boost_rounds))\n        #     if mean_merror < min_merror:\n        #         min_merror = mean_merror\n        #         best_params = (max_depth, min_child_weight)\n                \n        # print(\"Best params: {}, {}, min_merror: {}\".format(best_params[0], best_params[1], min_mae))\n        \n        # # Cross-validate max_depth/min_child_weight\n        # # These parameters control the complexity of the trees\n        # gridsearch_params = [(subsample, colsample) for subsample in [i/10. for i in range(7,11)] for colsample in [i/10. for i in range(7,11)]]\n        \n        classifier = xgb.train(\n            params,\n            xgb_train,\n            num_boost_round=num_boost_round,\n            evals=[(xgb_val, \"val\")],\n            early_stopping_rounds=early_stopping_rounds,\n            xgb_model=classifier # Continue training from previous epoch\n        )\n        \n        pred_val = classifier.predict(xgb_val)\n        correct = (pred_val==target_val).sum()\n        accuracy = 100. * correct/nval\n        \n        print('\\nValidation set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n            correct, nval, accuracy))\n\n    return (classifier, accuracy)\n    \n####################################\n# Functions used to make predictions\n####################################\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        with Image.open(f) as img:\n            return img.convert('RGB')\n            \ndef make_predictions(model, test_dir, outfile, classifier=None):\n    output_file = open(outfile, \"w\")\n    output_file.write(\"Id,Category\\n\")\n    print(\"Making predictions on the test set...\")\n    for f in os.listdir(test_dir):\n        if 'jpg' in f:\n            data = data_transforms(pil_loader(test_dir + '/' + f))\n            data = data.view(1, data.size(0), data.size(1), data.size(2))\n            if use_cuda:\n                data = data.cuda()\n                \n            if classifier == None:\n                output = model(data)\n                pred = output.data.max(1, keepdim=True)[1]\n            else:\n                output = model(data)\n                if type(classifier) == xgb.core.Booster:\n                    output = xgb.DMatrix(output.view(output.size()[0], -1).cpu().data.numpy())\n                pred = classifier.predict(output)\n                \n            output_file.write(\"%s,%d\\n\" % (f[:-4], pred))\n    \n    output_file.close()\n    print(\"Succesfully wrote \" + outfile + ', you can upload this file to the kaggle competition website\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d09bb17b1404ac0b7f86ac62e370dcc369dd887f"
      },
      "cell_type": "markdown",
      "source": "# 2. Cross-validation settings"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c86c7ee7c0db47352835129e4c65f74011142e4"
      },
      "cell_type": "code",
      "source": "# Creates random indices that will be used to cross-validate generalization error\ndata_loader = torch.utils.data.DataLoader(datasets.ImageFolder(data_dir, transform=data_transforms), \n    batch_size=batch_size, shuffle=True, num_workers=0)\n\nndata = len(data_loader.dataset)\nnfold = 10\nnval = np.int(ndata/nfold)\nntrain = ndata - nval\n\n# Generate splits\nrand_indices = np.random.permutation(ndata)\nlist_val_indices = [rand_indices[max((i-1)*nval,0):min(i*nval, ndata)] for i in range(1, nfold+1)]\nlist_train_indices = [list(set(rand_indices).difference(set(val_indices))) for val_indices in list_val_indices]\n\n# Normally if you define a sampler in the DataLoader function you need to set shuffle=False. However\n# using SubsetRandomSampler ensures shuffling itself even though shuffle=False\n# As a consequence validation set is shuffled too but it's no big deal\n\ntrain_indices = list_train_indices[0]\nval_indices = list_val_indices[0]\n\ntrain_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\nval_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "79426e82ef8b831702ceef7722eda09d13814da1"
      },
      "cell_type": "markdown",
      "source": "# 3. Run models"
    },
    {
      "metadata": {
        "_uuid": "39413b065e4f714a4ef2ae0aff203337cebd981a"
      },
      "cell_type": "markdown",
      "source": "## 3.1 Pretrained + default models"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3aa7ea9297bb4a20995384cccbdb56b42c399c28"
      },
      "cell_type": "code",
      "source": "#####################################################\n# Classifiers used as last layer of pretrained models\n#####################################################\n\ndef nn_1(in_features, num_classes):\n    return nn.Sequential(\n        nn.Linear(in_features, 4096),\n        nn.ReLU(inplace=True),\n        nn.Linear(4096, nclasses),\n    )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "e7293bb27154e07a12784c19aa62c292e39fb01a"
      },
      "cell_type": "code",
      "source": "# Let's use only train/val split to estimate quality of the model\n# First train complete CNN with pretrained models\n# Parameters of last layers of pretrained models are retrained\n\nclassifier_label = \"default\"\npretrained_models = {}\nvalidation_results = pd.DataFrame(columns=[\"pretrained\", \"classifier\", \"lr\", \"epoch\", \"accuracy\"])\n\nfor pretrained in [\"se_resnext50_32x4d\", \"inceptionresnetv2\", \"inception_v3\", \"densenet121\", \"se_resnet50\"]:\n    if not os.path.isdir(\"experiment/%s\" % pretrained):\n        os.mkdir(\"experiment/%s\" % pretrained)\n        \n    for classifier_label in [\"default\"]:\n        if classifier_label == \"default\":\n            model = make_model(pretrained, num_classes=nclasses, pretrained=True, input_size=(224, 224))\n        else:\n            model = make_model(pretrained, num_classes=nclasses, pretrained=True, input_size=(224, 224), classifier_factory=eval(classifier_label))\n            \n        # Because CUDA memory is limited, we can't update all parameters of pretrained models\n        if pretrained == \"densenet121\":\n            # Let weights of first 2 layers unchanged\n            for param in list(model.children())[0][0].parameters():\n                param.requires_grad = False\n            for param in list(model.children())[0][1].parameters():\n                param.requires_grad = False\n\n            # _Dense Block is composed of 6 dense layers\n            # Let weights of first 3 layers unchanged\n            for i in range(3):\n                for param in list(model.children())[0][4][i].parameters():\n                    param.requires_grad = False\n\n        elif pretrained == \"inception_v3\":\n            # First 7 layers of inception_v3 are BasicConv2d and MaxPool2D\n            # Next 4 are InceptionA layers\n            for i in range(11):\n                for param in list(model.children())[0][i].parameters():\n                    param.requires_grad = False\n\n        elif pretrained == \"se_resnet50\":\n            # CUDA memory is enough to update all parameters\n            pass\n        \n        elif pretrained == \"inceptionresnetv2\":\n            # First 15 layers, only update params of last one\n            for i in range(14):\n                for param in list(model.children())[0][i].parameters():\n                    param.requires_grad = False\n\n        print(\"#\"*40)\n        print(\"Model specifications\")\n        print(\"Pretrained: %s\" % pretrained)\n        print(\"Classifier: %s\" % classifier_label)\n        print(\"SGD learning rate: %s\" % lr)\n        if use_cuda:\n            print('Using GPU')\n            model.cuda()\n        else:\n            print('Using CPU')\n        print(\"#\"*40)\n        print(\"\\n\")\n\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n\n        # Data augmentation apply to each epoch\n        for epoch in range(1, nepoch + 1):\n            dataset = datasets.ImageFolder(data_dir, transform=data_transforms) # Data augmentation apply to each epoch\n            train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n            val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n\n            # Train the model on the selected epoch on the train dataset and then compute validation accuracy on val dataset\n            train(epoch, model, train_loader, ntrain, batch_size) # Use mini-batches of size batch_size\n            accuracy = validation(model, val_loader, nval)\n\n            row = {\"pretrained\": pretrained, \"classifier\": classifier_label, \"lr\": lr, \"epoch\": epoch, \"accuracy\": accuracy}\n            validation_results = validation_results.append(row, ignore_index=True)\n            validation_results.to_csv(\"../working/experiment/validation_results.csv\")\n\n            # Make predictions on test images\n            model.eval()\n            if use_cuda:\n                model.cuda()\n\n            outfile = \"../working/experiment/%s/kaggle_%s_%s_epoch%d.csv\" % (pretrained, pretrained, classifier_label, epoch)\n            make_predictions(model, test_dir, outfile)\n\n        # Save model with weights updated from training\n        pretrained_models[pretrained] = model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "23e4b9244049d841135f77cd9e598095d6a198dc"
      },
      "cell_type": "markdown",
      "source": "## 3.2 Other classifiers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a14dea6fef72bc4740c3e226d1e13797a9443e32"
      },
      "cell_type": "code",
      "source": "for pretrained, model in pretrained_models.items():\n    model = list(model.children())[0] # Retrieve pretrained model and discard last layers used for classification\n    for classifier_label in [\"xgboost\"]:\n        print(\"#\"*40)\n        print(\"Model specifications\")\n        print(\"Pretrained: %s\" % pretrained)\n        print(\"Classifier: %s\" % classifier_label)\n        print(\"#\"*40)\n        print(\"\\n\")\n        \n        classifier = None\n        \n        dataset = datasets.ImageFolder(data_dir, transform=data_transforms) # Data augmentation apply to each epoch\n        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n        val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n\n        # Train the model on the selected epoch and then compute validation accuracy\n        classifier, accuracy = train_batch(model, classifier, classifier_label, train_loader, val_loader, nval)\n        row = {\"pretrained\": pretrained, \"classifier\": classifier_label, \"lr\": -1, \"epoch\": epoch, \"accuracy\"\n        : accuracy}\n        validation_results = validation_results.append(row, ignore_index=True)\n        validation_results.to_csv(\"../working/experiment/validation_results.csv\")\n\n        # Make predictions on test images\n        model.eval()\n        if use_cuda:\n            model.cuda()\n\n        outfile = \"../working/experiment/%s/kaggle_%s_%s_epoch%d.csv\" % (pretrained, pretrained, classifier_label, epoch)\n        make_predictions(model, test_dir, outfile, classifier)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c9e3190eab0d0f9191545e0e30973bde0c298ce8"
      },
      "cell_type": "markdown",
      "source": "## 3.3 Ensemble technique: MajorityVote Classifier"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c406d989490373bbeb6a16fc082ddb86e0bf6baa"
      },
      "cell_type": "code",
      "source": "validation_results",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4617008a540e1a88119b87d3ad0dafb6c6d359a1"
      },
      "cell_type": "code",
      "source": "from scipy.stats import mode\n\n######################################################\n# Code that mixes predictions of the differents models\n# One technique is used majority vote to classify\n######################################################\n\npath = \"../working/experiment/\"\nthreshold = 80\n\n# Read output files\nvals = validation_results\noutput = pd.read_csv(path + \"densenet121/kaggle_densenet121_default_epoch1.csv\", usecols=[\"Id\"])\n        \n# Select predictions of models having validation performance above threshold\noutputs_model = pd.DataFrame.copy(output)\ndel outputs_model[\"Id\"]\nfor pretrained in vals.pretrained.unique():\n    for classifier in vals.classifier.unique():\n        for epoch in vals.epoch.unique():\n            mask = (vals.pretrained == pretrained) & (vals.classifier == classifier) & (vals.epoch == epoch)\n            if mask.sum() > 0:\n                accuracy = vals[mask][\"accuracy\"].values[0]\n                name = \"Category_%s_%s_%s\" % (pretrained, classifier, epoch)\n\n                outfile = \"%s/kaggle_%s_%s_epoch%d.csv\" % (pretrained, pretrained, classifier, epoch)\n                out = pd.read_csv(path  + outfile)\n\n                if accuracy >= threshold:\n                    outputs_model.loc[:, name] = out[\"Category\"]\n                \noutput.loc[:, \"Category\"] = outputs_model.apply(lambda x: mode(x)[0][0], axis=1)\noutput.to_csv(\"output.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}